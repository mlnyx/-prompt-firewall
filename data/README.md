---
license: cc-by-nc-4.0
tags:
- prompt
- injection
- jailbreak
- benign
---

# Dataset: Qualifire Benchmark Prompt Injection(Jailbreak vs. Benign) Datasets

## Overview
This dataset contains **5,000 prompts**, each labeled as either `jailbreak` or `benign`. The dataset is designed for evaluating AI models' robustness against adversarial prompts and their ability to distinguish between safe and unsafe inputs.

## Dataset Structure
- **Total Samples:** 5,000
- **Labels:** `jailbreak`, `benign`
- **Columns:** 
  - `text`: The input text
  - `label`: The classification (`jailbreak` or `benign`)


## License
cc-by-nc-4.0
